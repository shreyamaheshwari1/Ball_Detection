{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-6z8jYiW0RO"
      },
      "source": [
        "### Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "os6e5YfoW3Si",
        "outputId": "ef61e177-6f54-455f-c5c1-0545b839e5ee"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCZ89e37XEfN"
      },
      "outputs": [],
      "source": [
        "root = \"drive/MyDrive/DEP\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgRVHXh_CjIq"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ufKQ1dYmPjZ",
        "outputId": "f32a8b27-7ccb-47fe-a543-206f86fb6bba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in c:\\users\\dell2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.6.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\dell2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\dell2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\dell2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\dell2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\dell2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\dell2\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import pickle\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Dataset , DataLoader, TensorDataset\n",
        "import torch.nn.functional as func\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from pdb import set_trace as stx\n",
        "import numbers\n",
        "\n",
        "!pip install einops\n",
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63YAWfCRJWkA"
      },
      "source": [
        "### Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zcomXq3IDFB"
      },
      "outputs": [],
      "source": [
        "## Layer Norm\n",
        "def to_3d(x):\n",
        "    return rearrange(x, 'b c h w -> b (h w) c')\n",
        "\n",
        "def to_4d(x,h,w):\n",
        "    return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)\n",
        "\n",
        "class BiasFree_LayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape):\n",
        "        super(BiasFree_LayerNorm, self).__init__()\n",
        "        if isinstance(normalized_shape, numbers.Integral):\n",
        "            normalized_shape = (normalized_shape,)\n",
        "        normalized_shape = torch.Size(normalized_shape)\n",
        "\n",
        "        assert len(normalized_shape) == 1\n",
        "\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
        "        return x / torch.sqrt(sigma+1e-5) * self.weight\n",
        "\n",
        "class WithBias_LayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape):\n",
        "        super(WithBias_LayerNorm, self).__init__()\n",
        "        if isinstance(normalized_shape, numbers.Integral):\n",
        "            normalized_shape = (normalized_shape,)\n",
        "        normalized_shape = torch.Size(normalized_shape)\n",
        "\n",
        "        assert len(normalized_shape) == 1\n",
        "\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu = x.mean(-1, keepdim=True)\n",
        "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
        "        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim, LayerNorm_type):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        if LayerNorm_type =='BiasFree':\n",
        "            self.body = BiasFree_LayerNorm(dim)\n",
        "        else:\n",
        "            self.body = WithBias_LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, w = x.shape[-2:]\n",
        "        return to_4d(self.body(to_3d(x)), h, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GkasCl7H9aR"
      },
      "outputs": [],
      "source": [
        "## Gated-Dconv Feed-Forward Network (GDFN)\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
        "        super(FeedForward, self).__init__()\n",
        "\n",
        "        hidden_features = int(dim*ffn_expansion_factor)\n",
        "\n",
        "        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
        "\n",
        "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
        "\n",
        "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.project_in(x)\n",
        "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
        "        x = F.gelu(x1) * x2\n",
        "        x = self.project_out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21jxXk2sH5ny"
      },
      "outputs": [],
      "source": [
        "## Multi-DConv Head Transposed Self-Attention (MDTA)\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, bias):\n",
        "        super(Attention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
        "\n",
        "        self.qkv = nn.Conv2d(dim, dim*3, kernel_size=1, bias=bias)\n",
        "        self.qkv_dwconv = nn.Conv2d(dim*3, dim*3, kernel_size=3, stride=1, padding=1, groups=dim*3, bias=bias)\n",
        "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        b,c,h,w = x.shape\n",
        "\n",
        "        qkv = self.qkv_dwconv(self.qkv(x))\n",
        "        q,k,v = qkv.chunk(3, dim=1)\n",
        "\n",
        "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
        "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
        "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
        "\n",
        "        q = torch.nn.functional.normalize(q, dim=-1)\n",
        "        k = torch.nn.functional.normalize(k, dim=-1)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        out = (attn @ v)\n",
        "\n",
        "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
        "\n",
        "        out = self.project_out(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yxPqfMSHn6Q"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, ffn_expansion_factor, bias=False, LayerNorm_type='BiasFree'):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.norm1 = LayerNorm(dim, LayerNorm_type)\n",
        "        self.attn = Attention(dim, num_heads, bias)\n",
        "        self.norm2 = LayerNorm(dim, LayerNorm_type)\n",
        "        self.ffn = FeedForward(dim, ffn_expansion_factor, bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.ffn(self.norm2(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQSHu_1RJb6T"
      },
      "source": [
        "### Sub-Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaBvzEpXIxUQ"
      },
      "outputs": [],
      "source": [
        "class B_Block(nn.Module):\n",
        "    def __init__(self, ins, outs):\n",
        "        super().__init__()\n",
        "        self.C1 = nn.Sequential(nn.Conv2d(in_channels=ins, out_channels=ins, kernel_size=3, stride=1, padding=1),\n",
        "                                nn.BatchNorm2d(ins),\n",
        "                                nn.ReLU()\n",
        "                                )\n",
        "        self.C2 = nn.Sequential(nn.Conv2d(in_channels=ins, out_channels=ins, kernel_size=3, stride=1, padding=1),\n",
        "                                nn.BatchNorm2d(ins),\n",
        "                                nn.ReLU()\n",
        "                                )\n",
        "        self.C3 = nn.Sequential(nn.Conv2d(in_channels=ins, out_channels=ins, kernel_size=3, stride=2, padding=1),\n",
        "                                nn.BatchNorm2d(outs),\n",
        "                                nn.ReLU()\n",
        "                                )\n",
        "        self.tr = TransformerBlock(32, 8, 2.66)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.C1(x)\n",
        "        out2 = self.C2(out1)\n",
        "        out2 = out2 + x\n",
        "        out3 = self.C3(out2)\n",
        "        outf = self.tr(out3)\n",
        "        return outf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZAC7fx8I0w2"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, ins, outs):\n",
        "        super().__init__()\n",
        "        self.C1 = nn.Sequential(nn.Conv2d(in_channels=ins, out_channels=ins, kernel_size=3, stride=1, padding=1),\n",
        "                                nn.BatchNorm2d(ins),\n",
        "                                nn.ReLU()\n",
        "                                )\n",
        "        self.C2 = nn.Sequential(nn.Conv2d(in_channels=ins, out_channels=ins, kernel_size=3, stride=1, padding=1),\n",
        "                                nn.BatchNorm2d(ins),\n",
        "                                nn.ReLU()\n",
        "                                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.C1(x)\n",
        "        out2 = self.C2(out1)\n",
        "        outf = out2 + x\n",
        "        return outf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6_8OxVEI3nY"
      },
      "outputs": [],
      "source": [
        "class D_Block(nn.Module):\n",
        "    def __init__(self, ins, outs):\n",
        "        super().__init__()\n",
        "        self.C1 = nn.Sequential(nn.ConvTranspose2d(in_channels=ins, out_channels=ins, kernel_size=3, stride=1, padding=1),\n",
        "                                nn.BatchNorm2d(ins),\n",
        "                                nn.ReLU()\n",
        "                                )\n",
        "        self.C2 = nn.Sequential(nn.ConvTranspose2d(in_channels=ins, out_channels=ins, kernel_size=3, stride=1, padding=1),\n",
        "                                nn.BatchNorm2d(ins),\n",
        "                                nn.ReLU()\n",
        "                                )\n",
        "        self.C3 = nn.Sequential(nn.ConvTranspose2d(in_channels=ins, out_channels=ins, kernel_size=(3, 4), stride=(1, 2), padding=1),\n",
        "                                nn.BatchNorm2d(outs),\n",
        "                                nn.ReLU()\n",
        "                                )\n",
        "        self.tr = TransformerBlock(32, 8, 2.66)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.C1(x)\n",
        "        out2 = self.C2(out1)\n",
        "        out2 = out2 + x\n",
        "        out3 = self.C3(out2)\n",
        "        outf = self.tr(out3)\n",
        "        return outf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwp45Of88of7",
        "outputId": "e3d79f39-8e11-436c-e4f0-d1a278b2b2dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 32, 36, 18])\n",
            "torch.Size([20, 32, 36, 36])\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have a batch of images\n",
        "batch_size = 20\n",
        "channels = 32\n",
        "height_in = 36\n",
        "width_in = 18\n",
        "\n",
        "input_images = torch.rand((batch_size, channels, height_in, width_in))\n",
        "print(input_images.size())\n",
        "\n",
        "conv_transpose = nn.ConvTranspose2d(in_channels=channels, out_channels=channels, kernel_size=(3, 4), stride=(1, 2), padding=1)\n",
        "\n",
        "output_images = conv_transpose(input_images)\n",
        "\n",
        "print(output_images.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeouJyNSJhvv"
      },
      "source": [
        "### Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v55Vvqevijep"
      },
      "outputs": [],
      "source": [
        "def concatenate(in1, in2, dim=2):\n",
        "    return torch.cat([in1, in2], dim=dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P7MtWYbq4Km"
      },
      "outputs": [],
      "source": [
        "class FinalModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FinalModel, self).__init__()\n",
        "\n",
        "        self.cin1 = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "                                nn.BatchNorm2d(32),\n",
        "                                nn.ReLU()\n",
        "                                )\n",
        "        self.B11 = B_Block(32, 32)\n",
        "        self.B21 = B_Block(32, 32)\n",
        "        self.B31 = B_Block(32, 32)\n",
        "        self.R11 = Residual(32, 32)\n",
        "        self.R21 = Residual(32, 32)\n",
        "\n",
        "        self.cin2 = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "                                nn.BatchNorm2d(32),\n",
        "                                nn.ReLU()\n",
        "                                )\n",
        "        self.B12 = B_Block(32, 32)\n",
        "        self.B22 = B_Block(32, 32)\n",
        "        self.B32 = B_Block(32, 32)\n",
        "        self.R12 = Residual(32, 32)\n",
        "        self.R22 = Residual(32, 32)\n",
        "\n",
        "        self.D1 = D_Block(32, 32)\n",
        "        self.D2 = D_Block(32, 32)\n",
        "        self.D3 = D_Block(32, 32)\n",
        "        self.cout = nn.Sequential(nn.Conv2d(in_channels=32, out_channels=3, kernel_size=3, stride=1, padding=1),\n",
        "                                nn.BatchNorm2d(3),\n",
        "                                nn.ReLU()\n",
        "                                )\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2): # (bx3x72x72)\n",
        "        in11 = self.cin1(x1)     # (bx32x72x72)\n",
        "        in21 = self.B11(in11)    # (bx32x36x36)\n",
        "        in31 = self.B21(in21)    # (bx32x18x18)\n",
        "        in41 = self.B31(in31)    # (bx32x9x9)\n",
        "        in51 = self.R11(in41)    # (bx32x9x9)\n",
        "        in61 = self.R21(in51)    # (bx32x9x9)\n",
        "\n",
        "        in12 = self.cin2(x2)     # (bx32x72x72)\n",
        "        in22 = self.B12(in12)    # (bx32x36x36)\n",
        "        in32 = self.B22(in22)    # (bx32x18x18)\n",
        "        in42 = self.B32(in32)    # (bx32x9x9)\n",
        "        in52 = self.R12(in42)    # (bx32x9x9)\n",
        "        in62 = self.R22(in52)    # (bx32x9x9)\n",
        "\n",
        "        addr = in61 + in62                  # (bx32x9x9)\n",
        "        addb3 = in41 + in42                 # (bx32x9x9)\n",
        "        con1 = concatenate(addr, addb3)     # (bx32x18x9)\n",
        "        out1 = self.D1(con1)                # (bx32x18x18)\n",
        "\n",
        "        addb2 = in31 + in32                 # (bx32x18x18)\n",
        "        con2 = concatenate(out1, addb2)     # (bx32x36x18)\n",
        "        out2 = self.D2(con2)                # (bx32x36x36)\n",
        "\n",
        "        addb1 = in21 + in22                 # (bx32x36x36)\n",
        "        con3 = concatenate(out2, addb1)     # (bx32x72x36)\n",
        "        out3 = self.D3(con3)                # (bx32x72x72)\n",
        "\n",
        "        outf = self.cout(out3)   # (bx3x72x72)\n",
        "\n",
        "        return outf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pulhM31v5kMD"
      },
      "outputs": [],
      "source": [
        "PatchModel = FinalModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Old Training Dataset"
      ],
      "metadata": {
        "id": "VwA51A0k2qv4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3qhD6FCHH-z"
      },
      "outputs": [],
      "source": [
        "# Program To Read video and Extract Frames\n",
        "def FrameCapture(path):\n",
        "\tvidObj = cv2.VideoCapture(path)\n",
        "\tsuccess = 1\n",
        "\tframes = []\n",
        "\t# h, w, c = 1080, 1120, 3\n",
        "\twhile success:\n",
        "\t\tsuccess, image = vidObj.read()\n",
        "\t\timage = np.array(image)\n",
        "\t\t# if( image.shape == (h, w, c)) frames.append(image)\n",
        "\t\tframes.append(image)\n",
        "\treturn frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "ejZP3DLs8A68",
        "outputId": "f37feaf3-3d76-42ce-d1fd-8772654c22e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['all.mp4']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3e8313934686>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-e237efadf412>\u001b[0m in \u001b[0;36mFrameCapture\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# h, w, c = 1080, 1120, 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvidObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                 \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;31m# if( image.shape == (h, w, c)) frames.append(image)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "data_folder = root+\"/train_videos\"\n",
        "videos_list = [v for v in os.listdir(data_folder) if v.lower().endswith(('.mp4'))]\n",
        "dataset = []\n",
        "print(videos_list)\n",
        "\n",
        "for video_name in videos_list:\n",
        "    video_path = os.path.join(data_folder, video_name)\n",
        "\n",
        "    frames = FrameCapture(video_path)\n",
        "    print(video_path, len(frames))\n",
        "\n",
        "    for frame in frames:\n",
        "        dataset.append(frame)\n",
        "\n",
        "# dataset = np.array(dataset)\n",
        "print(len(dataset))\n",
        "# print(dataset[0].shape)\n",
        "print(type(dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBeE94UwfJAR"
      },
      "outputs": [],
      "source": [
        "def get_arranged_frames(dataset):\n",
        "    h, w, c = dataset[0].shape\n",
        "    crop_width = int(w/2)\n",
        "\n",
        "    arranged_dataset = []\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        # print(i)\n",
        "        if(i==0):\n",
        "            img1 = dataset[i]\n",
        "            img2 = dataset[i]\n",
        "        else:\n",
        "            img1 = dataset[i-1]\n",
        "            img2 = dataset[i]\n",
        "\n",
        "        if(img1.shape != (h, w, c) or img2.shape != (h, w, c)):\n",
        "            continue\n",
        "\n",
        "        # print(img1.shape)\n",
        "        img1 = np.transpose(img1, (2, 1, 0))\n",
        "        img1 = img1[:, 0:crop_width, :]\n",
        "\n",
        "        img2 = np.transpose(img2, (2, 1, 0))\n",
        "        img2, label2 = img2[:, 0:crop_width, :], img2[:, crop_width:w, :]\n",
        "        # print(label2.shape)\n",
        "        # print(img1.shape)\n",
        "\n",
        "        # print(type(img1))\n",
        "        img1 = img1.astype(np.float32)\n",
        "        img2 = img2.astype(np.float32)\n",
        "        label2 = label2.astype(np.float32)\n",
        "\n",
        "        label2 = np.where(label2 <= 100, 0, label2)  # Set values less than 50 to 0\n",
        "        label2 = np.where(label2 > 100, 1, label2)  # Set values greater than 150 to 1\n",
        "\n",
        "        img1 = torch.tensor(img1)\n",
        "        img2 = torch.tensor(img2)\n",
        "        label2 = torch.tensor(label2)\n",
        "\n",
        "        arranged_dataset.append((img1, img2, label2))\n",
        "    return arranged_dataset\n",
        "\n",
        "tensor_dataset = get_arranged_frames(dataset)\n",
        "print(len(tensor_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### New Training Dataset"
      ],
      "metadata": {
        "id": "miZT59ro1pPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Program To Read video and Extract Frames\n",
        "def FrameCapture(path):\n",
        "\tvidObj = cv2.VideoCapture(path)\n",
        "\tsuccess = 1\n",
        "\tframes = []\n",
        "\t# h, w, c = 1080, 1120, 3\n",
        "\twhile success:\n",
        "\t\tsuccess, image = vidObj.read()\n",
        "\t\timage = np.array(image)\n",
        "\t\t# if( image.shape == (h, w, c)) frames.append(image)\n",
        "\t\tframes.append(image)\n",
        "\treturn frames"
      ],
      "metadata": {
        "id": "DMGbG9Kc1355"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video1 = root+\"/new_train_videos/patch1_video1.mp4\"\n",
        "video2 = root+\"/new_train_videos/concatenated1.mp4\"\n",
        "\n",
        "dataset1 = FrameCapture(video1)\n",
        "print(len(dataset1))\n",
        "\n",
        "dataset2 = FrameCapture(video2)\n",
        "print(len(dataset2))\n",
        "\n",
        "video1 = root+\"/new_train_videos/patch1_video2.mp4\"\n",
        "video2 = root+\"/new_train_videos/concatenated2.mp4\"\n",
        "\n",
        "temp = FrameCapture(video1)\n",
        "for frame in temp:\n",
        "    dataset1.append(frame)\n",
        "print(len(dataset1))\n",
        "\n",
        "temp = FrameCapture(video2)\n",
        "for frame in temp:\n",
        "    dataset2.append(frame)\n",
        "print(len(dataset2))"
      ],
      "metadata": {
        "id": "l5ytdp6A121g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f6b747f-29bc-41ca-9053-a50d4aaf684e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2360\n",
            "2359\n",
            "4592\n",
            "4590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_arranged_frames(dataset1, dataset2):\n",
        "    h, w, c = dataset2[0].shape\n",
        "    print(dataset1[0].shape)\n",
        "    print(dataset2[0].shape)\n",
        "\n",
        "    crop_width = int(w/2)\n",
        "\n",
        "    arranged_dataset = []\n",
        "\n",
        "    for i in range(len(dataset2)):\n",
        "        # print(i)\n",
        "        img1 = dataset1[i]\n",
        "        img2 = dataset2[i]\n",
        "\n",
        "        if(img1.shape != (h, int(w/2), c) or img2.shape != (h, w, c)):\n",
        "            continue\n",
        "\n",
        "        # print(img1.shape)\n",
        "        # img1 = np.transpose(img1, (2, 1, 0))\n",
        "        # img2 = np.transpose(img2, (2, 1, 0))\n",
        "        # img2, label2 = img2[:, 0:crop_width, :], img2[:, crop_width:w, :]\n",
        "\n",
        "        img1 = np.transpose(img1, (2, 0, 1))\n",
        "        img2 = np.transpose(img2, (2, 0, 1))\n",
        "        img2, label2 = img2[:, :, 0:crop_width], img2[:, :, crop_width:w]\n",
        "        # print(label2.shape)\n",
        "        # print(img1.shape)\n",
        "\n",
        "        # print(type(img1))\n",
        "        img1 = img1.astype(np.float32)\n",
        "        img2 = img2.astype(np.float32)\n",
        "        label2 = label2.astype(np.float32)\n",
        "\n",
        "        label2 = np.where(label2 <= 100, 0, label2)  # Set values less than 50 to 0\n",
        "        label2 = np.where(label2 > 100, 1, label2)  # Set values greater than 150 to 1\n",
        "\n",
        "        img1 = torch.tensor(img1)\n",
        "        img2 = torch.tensor(img2)\n",
        "        label2 = torch.tensor(label2)\n",
        "        # print(type(img1))\n",
        "\n",
        "        arranged_dataset.append((img1, img2, label2))\n",
        "    return arranged_dataset\n",
        "\n",
        "tensor_dataset = get_arranged_frames(dataset1, dataset2)"
      ],
      "metadata": {
        "id": "A1WSO-uc2A8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce18c3d-9a11-449e-bc77-a789e6d22867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(72, 72, 3)\n",
            "(72, 144, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrrg75Ir8DA7"
      },
      "source": [
        "### Dataset Preparation for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPAxiJG6JV5F",
        "outputId": "b8166959-5765-4f1e-bef1-d53761e30609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4587\n",
            "<class 'list'>\n",
            "3\n",
            "<class 'tuple'>\n",
            "3\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([3, 72, 72]) torch.Size([3, 72, 72]) torch.Size([3, 72, 72])\n"
          ]
        }
      ],
      "source": [
        "# tensor_dataset = np.array(tensor_dataset)\n",
        "\n",
        "print(len(tensor_dataset))\n",
        "print(type(tensor_dataset))\n",
        "print(len(tensor_dataset[0]))\n",
        "print(type(tensor_dataset[0]))\n",
        "print(len(tensor_dataset[0][0]))\n",
        "print(type(tensor_dataset[0][0]))\n",
        "# print(tensor_dataset.shape)\n",
        "# print(tensor_dataset[0].shape)\n",
        "print(tensor_dataset[0][0].shape, tensor_dataset[0][1].shape, tensor_dataset[0][2].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVNdCVBIta_3"
      },
      "outputs": [],
      "source": [
        "img1_list, img2_list, label_list = zip(*tensor_dataset)\n",
        "\n",
        "# Convert the lists to PyTorch tensors\n",
        "img1_tensor = torch.stack(img1_list)\n",
        "img2_tensor = torch.stack(img2_list)\n",
        "label_tensor = torch.stack(label_list)\n",
        "\n",
        "# Create a TensorDataset\n",
        "tensor_dataset = TensorDataset(img1_tensor, img2_tensor, label_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExNj3TuJk9ek",
        "outputId": "b00e6883-ab5c-4c6b-d511-bff2c610ad47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3670\n",
            "<class 'torch.utils.data.dataset.Subset'>\n",
            "917\n",
            "<class 'torch.utils.data.dataset.Subset'>\n",
            "<class 'tuple'>\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "torch.Size([3, 72, 72]) torch.Size([3, 72, 72]) torch.Size([3, 72, 72])\n",
            "(tensor([[[ 83.,  85.,  85.,  ...,  81.,  82.,  82.],\n",
            "         [ 82.,  83.,  83.,  ...,  81.,  82.,  82.],\n",
            "         [ 82.,  83.,  83.,  ...,  81.,  82.,  82.],\n",
            "         ...,\n",
            "         [ 83.,  83.,  83.,  ...,  85.,  85.,  85.],\n",
            "         [ 81.,  81.,  81.,  ...,  85.,  85.,  85.],\n",
            "         [ 81.,  81.,  81.,  ...,  85.,  85.,  85.]],\n",
            "\n",
            "        [[117., 119., 119.,  ..., 116., 117., 117.],\n",
            "         [116., 117., 117.,  ..., 116., 117., 117.],\n",
            "         [116., 117., 117.,  ..., 116., 117., 117.],\n",
            "         ...,\n",
            "         [117., 117., 117.,  ..., 121., 120., 120.],\n",
            "         [117., 117., 117.,  ..., 121., 120., 120.],\n",
            "         [117., 117., 117.,  ..., 121., 120., 120.]],\n",
            "\n",
            "        [[110., 112., 112.,  ..., 106., 107., 107.],\n",
            "         [109., 110., 110.,  ..., 106., 107., 107.],\n",
            "         [109., 110., 110.,  ..., 106., 107., 107.],\n",
            "         ...,\n",
            "         [110., 110., 110.,  ..., 109., 110., 110.],\n",
            "         [110., 110., 110.,  ..., 109., 110., 110.],\n",
            "         [110., 110., 110.,  ..., 109., 110., 110.]]]), tensor([[[ 86.,  86.,  86.,  ...,  82.,  82.,  82.],\n",
            "         [ 86.,  86.,  86.,  ...,  82.,  82.,  82.],\n",
            "         [ 86.,  86.,  86.,  ...,  82.,  82.,  82.],\n",
            "         ...,\n",
            "         [ 83.,  83.,  83.,  ...,  87.,  85.,  84.],\n",
            "         [ 83.,  83.,  83.,  ...,  88.,  85.,  84.],\n",
            "         [ 82.,  82.,  82.,  ...,  88.,  85.,  84.]],\n",
            "\n",
            "        [[120., 120., 120.,  ..., 116., 116., 116.],\n",
            "         [120., 120., 120.,  ..., 116., 116., 116.],\n",
            "         [120., 120., 120.,  ..., 116., 116., 116.],\n",
            "         ...,\n",
            "         [118., 118., 118.,  ..., 120., 117., 116.],\n",
            "         [117., 117., 117.,  ..., 121., 117., 116.],\n",
            "         [116., 116., 116.,  ..., 121., 117., 116.]],\n",
            "\n",
            "        [[115., 115., 115.,  ..., 111., 111., 111.],\n",
            "         [115., 115., 115.,  ..., 111., 111., 111.],\n",
            "         [115., 115., 115.,  ..., 111., 111., 111.],\n",
            "         ...,\n",
            "         [108., 108., 108.,  ..., 110., 110., 109.],\n",
            "         [110., 110., 110.,  ..., 111., 110., 109.],\n",
            "         [109., 109., 109.,  ..., 111., 110., 109.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]))\n"
          ]
        }
      ],
      "source": [
        "val_size = int(0.2*(len(tensor_dataset)))\n",
        "train_size = len(tensor_dataset) - val_size\n",
        "\n",
        "train_ds, val_ds = random_split(tensor_dataset, [train_size, val_size])\n",
        "\n",
        "print(len(train_ds))\n",
        "print(type(train_ds))\n",
        "print(len(val_ds))\n",
        "print(type(val_ds))\n",
        "print(type(val_ds[0]))\n",
        "# print(val_ds[0].shape)\n",
        "print(type(val_ds[0][0]), type(val_ds[0][1]), type(val_ds[0][2]))\n",
        "print(val_ds[0][0].shape, val_ds[0][1].shape, val_ds[0][2].shape)\n",
        "\n",
        "print(val_ds[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFDU0UABNXbV",
        "outputId": "1d29d887-8b30-4918-df94-e68e25bc3f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 72, 72])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 72, 72])\n",
            "torch.Size([32, 3, 72, 72])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcIAAAFICAYAAADDM/77AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAN00lEQVR4nO3cz4vd9X7H8fdJzlASIyEpFyfR3lIXakIVVxotVF240ZlkZtFuulB0YyxcKNeF/4GC7nShFGN/SHeJk8kstFAtGJ2km1JhJqNGaGsgiWC0RpOSOWe+XeggtMV7Jibf70xejwdkN5HPcT74PO/39xx7TdM0BQChNnV9AADokhACEE0IAYgmhABEE0IAogkhANGEEIBoQghANCEEIFp/1B/s9XrX8xwAcM2N8j9PGzmEVVW7d++uffv2XfWBNpqPPvqozp8/X1NTUzFvBM6ePVvz8/P14IMP1vj4eNfHaUXTNDUzM1O7du2Kut/z8/N17tw59/sGt3q/x8fH64EHHuj6OK2Zn5+vs2fPjvbDzYiqqpmenh71x28IExMTzdjYWDMYDLo+SmtmZmaaqmrm5ua6Pkprrly50vT7/WZqaqrro7RqcnIy7n4fPXq0qapmdna266O0Znl5uen3+82BAwe6Pkqr9u/f34yaOM8Iidfr9UZanwA3JiEkXtM0MatB4P8SQgCiCSEA0YQQgGhCCEA0IQQgmhASz9cnIJsQEs/XJyCbEAIQTQiJZzUK2YSQeFajkE0IAYgmhMSzGoVsQkg8q1HIJoQARBNC4lmNQjYhBCCaEBLPM0LIJoQARBNC4nlGCNmEEIBoQkg8zwjb931V/UvXh4AfCSHxrEbbd6Wq/r3rQ8CPhJB4JsL27aiqP+/6EPAjISSeiRCyCSHxTISQTQgBiCaExLMahWxCSDyrUcgmhMQzEUI2ISSeiRCyCSHxTISQTQiJZyKEbEIIQDQhJJ7VKGQTQuJZjUI2IQQgmhACEE0IiecZIWQTQuJ5RgjZhBCAaEIIQDQhJJ5nhJBNCInnGSFkE0LimQghmxASz0QI2YSQeCZCyCaExDMRQjYhJJ6JELIJIfFMhJBNCIlnIoRsQkg8EyFkE0IAogkh8axGIZsQEs9qFLIJIQDRhJB4VqOQrdeM+F+AXq9XO3furLvuuut6n2ndOHXqVH3zzTe1b9++mNXZhQsXamlpqfbs2VM7duzo+jitaJqm5ufn3e8Aqff7xIkTtWPHjqj7vbS0VBcuXBjpTe6aQtjv9+umm276xQfcKL7//vsaDAa1ffv2ro/SmuXl5bp06VJt3bq1xsbGuj5OK5qmqW+//db9DuB+593vax7C6enpOnz48C8+4Eaxf//+evfdd+vy5cu1aVPGFnl2drampqZqbm6uHnvssa6P04rBYFBbt26tiYmJOnLkSNfHac2BAwfqnXfeibrfx44dqwMHDtTs7GxNTEx0fZxWDIfD2rJlSz3++OP19ttvd32c1kxNTdXs7OxIIeyv9R+eskL53xJft9ecI/F1e82syngbCD/Dh2UgmxASz/cIIZsQEs9ECNmEkHgmQsgmhMQzEUI2ISSeiRCyCSE/aX78AxBECPnJf1TVn3V9iPZZjUI2IeQnTVUNuj5E+6xGIZsQ8oOVqvqHMhECcYSQH6xU1d9W1V90fRCAdgkhP9hcVf/U9SG6YTUK2YSQH/Sq6g+6PgRA+4QQgGhCCEA0ISSeT41CNiEkng/LQDYhBCCaEAIQTQgBiCaEAEQTQgCiCSHxfH0Csgkh8Xx9ArIJIQDRhJB4VqOQTQiJZzUK2YQQgGhCSDyrUcgmhMSzGoVsQghANCEkntUoZBNCAKIJIfE8I4RsQgjrxPJy1W9+0/UpII8QEm+9PCPs96uee67rU0AeIYR1oter+vWvuz4F5BFC4nlGCNmEkHjrZTUKdEMIiWcihGxCSDwTIWQTQuKZCCGbEAIQTQiJZzUK2YSQeFajkE0IiWcihGxCSDwTIWQTQuKZCCGbEBLPRAjZhBCAaEJIPKtRyCaExLMahWxCCEA0IQQgmhASzzNCyCaExPOMELIJIQDR+mv54c8++6xeeuml63WWdefzzz+vlZWVevnll2vTpoz3DAsLC1VVdfjw4VpcXOz4NO1YWVmplZWVuPt9+vTpGg6HUfd79U4fOXKklpaWOj5NO1bv9+nTp+Pu96h6zYgPR6yOANhoRkncmibCRx99tF555ZWrPtBG88wzz9Tx48fr448/jnnH/N5779Wzzz5br7/+ej300ENdH6cVw+Gw7rnnnnrkkUfq1Vdf7fo4rTl48GB98MEHUff7/fffr4MHD9Zrr71WDz/8cNfHacXq/R4Oh10fZd1aUwi3bdtWd9555/U6y7qzbdu26vV6dccdd9TmzZu7Pk4rVtdFt956a8zvejAYVJX7neCTTz6pqqrdu3fH/K4Hg4GN3u+Q8TYQfoZPjUI2ISSe7xFCNiEknokQsgkh8UyEkE0IiWcihGxCSDwTIWQTQuKZCCGbEAIQTQiJZzUK2YSQeFajkE0IAYgmhMSzGoVsQkg8q1HIJoTEMxFCNiEknokQsgkhANGEkHhWo5BNCIlnNQrZhJB4JkLIJoTEMxFCNiEknokQsgkh8UyEkE0IAYgmhMSzGoVsQkg8q1HItqFD+NZbVS9+V+W9PL+EiRCybdgQ/k1V/dUfVd3W7/okAGxkGzYjk1X1p39S9YdVZanFL2E1Ctk2bAh//8c/APBLbNjVKABcC0IIQDQhJJ5PjUI2ISSeD8tANiEEIJoQAhBNCAGIJoQARBNCAKIJIfF8fQKyCSHxfH0CsgkhANGEkHhWo5BNCIlnNQrZhBCAaEJIPKtRyCaExLMahWxCCEA0ISSe1ShkE0IAogkh8TwjhGxCCEA0ISSeZ4SQTQgBiCaExPOMELIJIfGsRiGbEBLPRAjZhJB4JkLI1l/LD1+8eLEWFxev11nWnYsXL1bTNHXq1KnatCnjPcOZM2eqquqLL76I+V0Ph8Oqcr8TrN7vM2fOxPyuh8OhN3q/Q68Z8d+Q1REAG80oiVvTRHj33XfXE088cdUH2mjeeOON+vTTT+uFF16Iece8sLBQb775Zj399NO1Z8+ero/TipWVlXr++edr79699eSTT3Z9nNYcOnSolpaW6sUXX4y534uLi3Xo0KF66qmnau/evV0fpxWr93tlZaXro6xfzYiqqpmenh71x28IExMTzdjYWDMYDLo+SmtmZmaaqmrm5ua6Pkprrly50vT7/WZqaqrro7RqcnIy7n4fPXq0qapmdna266O0Znl5uen3+01VRf4ZRcbbQPgZPiwD2YSQeI2vT0A0ISSeiRCyCSHxTISQTQgBiCaExLMahWxCSDyrUcgmhABEE0IAogkh8TwjhGxCSDzPCCGbEAIQTQgBiCaExPOMELIJIfE8I4RsQkg8EyFkE0LimQghmxASz0QI2YSQeCZCyCaExDMRQjYhJJ6JELIJIfFMhJBNCIlnIoRsQghANCEkntUoZBNC4lmNQjYhBCCaEBLPahSyCSHxrEYhmxASz0QI2YSQeCZCyCaEAEQTQuJZjUI2ISSe1ShkE0LimQghmxASz0QI2YSQeCZCyCaExDMRQjYhBCCaEBLPahSyCSHxrEYhmxASz0QI2YQQgGhCSDyrUcgmhABEE0IAogkhANGEkHg+NQrZhJB4PiwD2YQQgGhCCEA0IQQgmhACEE0IAYgmhMTz9QnIJoTE8/UJyCaEAETrr/UvJK6QmqaJe91pr3n1tSa95lVpv+tVia+Z/1+vGfE29Hq96vf7tWXLlut9pnXj0qVLNRwO6+abb+76KK0ZDAZ1+fLl2rJlS/X7a36ftCE1TVPfffed+x0g+X6nGiVxa7oJ27dvr7179171gTaahYWF+vrrr+vee+/t+iit+eqrr2pxcbFuv/322rlzZ9fHaUXTNHX8+HH3O8CFCxdqYWEh7n5/+OGHJuCf04yoqprp6elRf/yGMDEx0YyNjTWDwaDro7RmZmamqapmbm6u66O0Znl5udm8eXMzNTXV9VFaNTk5GXe/jx492lRVMzs72/VRWrO8vNz0+/2mqiL/jMKHZYjX+NQoRBNCAKIJIfF8oR6yCSEA0YSQeJ4RQjYhBCCaEBLPM0LIJoQARBNC4nlGCNmEkHhWo5BNCIlnIoRsQkg8EyFkE0LimQghmxACEE0IiWc1CtmEkHhWo5BNCIlnIoRsQkg8EyFkE0LimQghmxASz0QI2YQQgGhCSDyrUcgmhMSzGoVsQghANCEEIJoQEs8zQsgmhMTzjBCyCSEA0YQQgGhCSDzPCCGbEBLPM0LIJoTEMxFCNiEknokQsvWv7q/9fVV9XlW/qqq/vIbHgfaZCCHbVYbwrar6x6raW0LIRmcihGxXGcK/q6r/rqqxa3kW6ISJELJdZQhvubangA6ZCCGbD8sQz0QI2YSQeCZCyCaEAEQTQuJZjUI2ISSe1ShkE0IAogkh8axGIZsQEs9qFLIJIfFMhJBNCIlnIoRsQghANCEkntUoZBNC4lmNQjYhJJ6JELIJIfFMhJBNCIlnIoRsQkg8EyFkE0IAogkh8axGIZsQEs9qFLL1uz4AdM1EuH6dr6q/rn+reudY1R9X/d5tv63nakt528K11GtG/C9Ar9erXbt21X333Xe9z7RunDhxor788suanJyMmRjOnTtXJ0+erH379tUtt9zS9XFa0TRNHTt2rMbHx6Pu98mTJ+v8+fPr+n7/V1X9c/1n1cf/WnVbVX/nY/V4jV31P2/1ft9///01Pj5+7Q66jq3e79Q3e6O87jWFEAA2klESN/JqNPXdBAA3Nh+WASCaEAIQTQgBiCaEAEQTQgCiCSEA0YQQgGhCCEA0IQQg2v8An7/dnLdczJ8AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_dl = DataLoader(val_ds, batch_size, num_workers=2, pin_memory=True)\n",
        "\n",
        "for batch in train_dl:\n",
        "    img1, img2, label2 = batch\n",
        "\n",
        "    print(img1.shape)\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(make_grid(img1, nrow=5).permute((2, 1, 0)))\n",
        "\n",
        "    print(img2.shape)\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(make_grid(img2, nrow=5).permute((2, 1, 0)))\n",
        "\n",
        "    print(label2.shape)\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(make_grid(label2, nrow=5).permute((2, 1, 0)))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6JDJ6cw8F8D"
      },
      "source": [
        "### Training PatchModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pLUJV8RMKr-"
      },
      "outputs": [],
      "source": [
        "def accuracy(pred_masks, true_masks):\n",
        "    pred_masks_binary = np.where(pred_masks.cpu().detach().numpy() > 0.5, 1, 0)\n",
        "\n",
        "    # print(pred_masks_binary.shape)\n",
        "    # print(true_masks.shape)\n",
        "\n",
        "    correct_pixels = np.sum(pred_masks_binary == true_masks.cpu().numpy())\n",
        "    total_pixels = np.prod(pred_masks.shape)\n",
        "\n",
        "    acc = correct_pixels / total_pixels\n",
        "\n",
        "    # print(correct_pixels, total_pixels, acc)\n",
        "    return torch.tensor(acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_loss(pred, target):\n",
        "    smooth = 1e-5\n",
        "    intersection = (pred * target).sum()\n",
        "    union = pred.sum() + target.sum() + smooth\n",
        "    return 1 - (2 * intersection) / union"
      ],
      "metadata": {
        "id": "hzbu2k5ph4-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4birjSTYhP5"
      },
      "outputs": [],
      "source": [
        "def training_step(model, batch):\n",
        "    input1, input2, target = batch\n",
        "    out = model(input1, input2)\n",
        "\n",
        "    sigmo = nn.Sigmoid()\n",
        "    out = sigmo(out)\n",
        "    # print(out.shape, target.shape)\n",
        "    assert out.shape == target.shape, \"Predictions and target labels must have the same shape\"\n",
        "\n",
        "    loss = dice_loss(out, target)\n",
        "    # loss = func.binary_cross_entropy(out, target)\n",
        "    return loss\n",
        "\n",
        "def validation_step(model, batch):\n",
        "    input1, input2, target = batch\n",
        "    out = model(input1, input2)                     # Generate predictions\n",
        "\n",
        "    sigmo = nn.Sigmoid()\n",
        "    out = sigmo(out)\n",
        "    assert out.shape == target.shape, \"Predictions and target labels must have the same shape\"\n",
        "    # lossfxn = nn.BCELoss()\n",
        "    # loss = lossfxn(out, target)\n",
        "    loss = dice_loss(out, target)\n",
        "    # loss = func.binary_cross_entropy(out, target)          # Calculate loss\n",
        "    acc = accuracy(out, target)                     # Calculate accuracy\n",
        "    return {'val_loss': loss.detach(), 'val_acc':acc}\n",
        "\n",
        "def validation_epoch_end(outputs):\n",
        "    batch_losses = [x['val_loss'] for x in outputs]\n",
        "    epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "    batch_accs = [x['val_acc'] for x in outputs]\n",
        "    epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "    return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "\n",
        "def epoch_end(epoch, result):\n",
        "    print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODx7oskJteBr"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [validation_step(model, batch) for batch in val_loader]\n",
        "    return validation_epoch_end(outputs)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def train(epochs, model, train_loader, val_loader, lr, opt_func=torch.optim.SGD):\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "    weight_decay = 0\n",
        "    optimizer = opt_func(model.parameters(), lr, weight_decay=weight_decay)\n",
        "\n",
        "    # Set up one-cycle learning rate scheduler\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, lr, epochs=epochs, steps_per_epoch=len(train_loader))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        lrs = []\n",
        "        for batch in train_loader:\n",
        "            loss = training_step(model, batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "\n",
        "            nn.utils.clip_grad_value_(model.parameters(), 0.1)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            sched.step()\n",
        "\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['lrs'] = lrs\n",
        "        epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGXnE_ENVjao"
      },
      "source": [
        "### **Loading the training, validation & test data, & model to the GPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVlJ-ZPWy6JO",
        "outputId": "59dedab2-0da2-429d-8f6b-2ded1850d78e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PopxxLvFy72D",
        "outputId": "ea354872-3938-47f1-ff06-d9442b00d159"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "def get_default_device():\n",
        "  ''' Function to select the available device '''\n",
        "  if torch.cuda.is_available():\n",
        "    return torch.device('cuda')\n",
        "  else:\n",
        "    return torch.device('cpu')\n",
        "\n",
        "device = get_default_device()\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvLUauUzy-BR"
      },
      "outputs": [],
      "source": [
        "def to_device(device, data):\n",
        "  ''' Function to move the tensor/data to the selected device '''\n",
        "  if isinstance(data, (list, tuple)):\n",
        "    return [to_device(device, x) for x in data]\n",
        "  return data.to(device, non_blocking=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEHFd-Osy_3B"
      },
      "outputs": [],
      "source": [
        "class DeviceDataLoader():\n",
        "  \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "  def __init__(self, DL, device):\n",
        "    self.DL = DL\n",
        "    self.device = device\n",
        "\n",
        "  def __iter__(self):\n",
        "    \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "    for batch in self.DL:\n",
        "      yield to_device(self.device, batch)\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"Number of batches\"\"\"\n",
        "    return len(self.DL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjGJ_-j9yM98"
      },
      "outputs": [],
      "source": [
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "val_dl = DeviceDataLoader(val_dl, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2BCft0oyrxT",
        "outputId": "7906db25-82b7-4921-e477-c193cf351c83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FinalModel(\n",
              "  (cin1): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (B11): B_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (B21): B_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (B31): B_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (R11): Residual(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (R21): Residual(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (cin2): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (B12): B_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (B22): B_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (B32): B_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (R12): Residual(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (R22): Residual(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (D1): D_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 4), stride=(1, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (D2): D_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 4), stride=(1, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (D3): D_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 4), stride=(1, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cout): Sequential(\n",
              "    (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "to_device(device, PatchModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f57MJ1cSyw37",
        "outputId": "f245b769-6db0-4667-e9c2-d63ec8fc2be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0 cuda:0 cuda:0\n",
            "cuda:0 cuda:0 cuda:0\n"
          ]
        }
      ],
      "source": [
        "for img1b, img2b, label2b in train_dl:\n",
        "  print(img1b.device, img2b.device, label2b.device)\n",
        "  break\n",
        "for img1b, img2b, label2b in val_dl:\n",
        "  print(img1b.device, img2b.device, label2b.device)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhQJ3a4czoQX"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzG90ariO9x-"
      },
      "outputs": [],
      "source": [
        "opt_func = torch.optim.Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VqpWNk4aQqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b5575b-9a14-4e36-f9e4-1843582fb9ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0], last_lr: 0.00005, train_loss: 0.9093, val_loss: 0.9120, val_acc: 0.9913\n",
            "Epoch [1], last_lr: 0.00008, train_loss: 0.9093, val_loss: 0.9119, val_acc: 0.9911\n",
            "Epoch [2], last_lr: 0.00013, train_loss: 0.9093, val_loss: 0.9120, val_acc: 0.9913\n",
            "Epoch [3], last_lr: 0.00020, train_loss: 0.9093, val_loss: 0.9118, val_acc: 0.9906\n",
            "Epoch [4], last_lr: 0.00028, train_loss: 0.9094, val_loss: 0.9120, val_acc: 0.9904\n",
            "Epoch [5], last_lr: 0.00037, train_loss: 0.9094, val_loss: 0.9118, val_acc: 0.9898\n",
            "Epoch [6], last_lr: 0.00047, train_loss: 0.9095, val_loss: 0.9118, val_acc: 0.9897\n",
            "Epoch [7], last_lr: 0.00057, train_loss: 0.9096, val_loss: 0.9117, val_acc: 0.9895\n",
            "Epoch [8], last_lr: 0.00067, train_loss: 0.9097, val_loss: 0.9124, val_acc: 0.9881\n",
            "Epoch [9], last_lr: 0.00076, train_loss: 0.9099, val_loss: 0.9117, val_acc: 0.9794\n",
            "Epoch [10], last_lr: 0.00084, train_loss: 0.9100, val_loss: 0.9124, val_acc: 0.9827\n",
            "Epoch [11], last_lr: 0.00091, train_loss: 0.9100, val_loss: 0.9114, val_acc: 0.9861\n",
            "Epoch [12], last_lr: 0.00096, train_loss: 0.9098, val_loss: 0.9119, val_acc: 0.9879\n",
            "Epoch [13], last_lr: 0.00099, train_loss: 0.9098, val_loss: 0.9119, val_acc: 0.9884\n",
            "Epoch [14], last_lr: 0.00100, train_loss: 0.9098, val_loss: 0.9116, val_acc: 0.9859\n",
            "Epoch [15], last_lr: 0.00100, train_loss: 0.9099, val_loss: 0.9116, val_acc: 0.9858\n",
            "Epoch [16], last_lr: 0.00099, train_loss: 0.9099, val_loss: 0.9117, val_acc: 0.9878\n",
            "Epoch [17], last_lr: 0.00098, train_loss: 0.9099, val_loss: 0.9114, val_acc: 0.9845\n",
            "Epoch [18], last_lr: 0.00097, train_loss: 0.9099, val_loss: 0.9115, val_acc: 0.9873\n",
            "Epoch [19], last_lr: 0.00095, train_loss: 0.9097, val_loss: 0.9113, val_acc: 0.9876\n",
            "Epoch [20], last_lr: 0.00093, train_loss: 0.9098, val_loss: 0.9133, val_acc: 0.9862\n",
            "Epoch [21], last_lr: 0.00090, train_loss: 0.9098, val_loss: 0.9117, val_acc: 0.9859\n",
            "Epoch [22], last_lr: 0.00088, train_loss: 0.9098, val_loss: 0.9118, val_acc: 0.9894\n",
            "Epoch [23], last_lr: 0.00085, train_loss: 0.9097, val_loss: 0.9116, val_acc: 0.9883\n",
            "Epoch [24], last_lr: 0.00081, train_loss: 0.9097, val_loss: 0.9113, val_acc: 0.9866\n",
            "Epoch [25], last_lr: 0.00078, train_loss: 0.9097, val_loss: 0.9117, val_acc: 0.9882\n",
            "Epoch [26], last_lr: 0.00074, train_loss: 0.9097, val_loss: 0.9118, val_acc: 0.9899\n",
            "Epoch [27], last_lr: 0.00070, train_loss: 0.9096, val_loss: 0.9117, val_acc: 0.9899\n",
            "Epoch [28], last_lr: 0.00065, train_loss: 0.9096, val_loss: 0.9120, val_acc: 0.9889\n",
            "Epoch [29], last_lr: 0.00061, train_loss: 0.9096, val_loss: 0.9119, val_acc: 0.9901\n",
            "Epoch [30], last_lr: 0.00057, train_loss: 0.9096, val_loss: 0.9118, val_acc: 0.9903\n",
            "Epoch [31], last_lr: 0.00052, train_loss: 0.9095, val_loss: 0.9115, val_acc: 0.9896\n",
            "Epoch [32], last_lr: 0.00048, train_loss: 0.9095, val_loss: 0.9118, val_acc: 0.9901\n",
            "Epoch [33], last_lr: 0.00043, train_loss: 0.9095, val_loss: 0.9116, val_acc: 0.9896\n",
            "Epoch [34], last_lr: 0.00039, train_loss: 0.9094, val_loss: 0.9118, val_acc: 0.9905\n",
            "Epoch [35], last_lr: 0.00035, train_loss: 0.9094, val_loss: 0.9116, val_acc: 0.9903\n",
            "Epoch [36], last_lr: 0.00030, train_loss: 0.9094, val_loss: 0.9121, val_acc: 0.9917\n",
            "Epoch [37], last_lr: 0.00026, train_loss: 0.9094, val_loss: 0.9117, val_acc: 0.9909\n",
            "Epoch [38], last_lr: 0.00022, train_loss: 0.9093, val_loss: 0.9119, val_acc: 0.9913\n",
            "Epoch [39], last_lr: 0.00019, train_loss: 0.9093, val_loss: 0.9118, val_acc: 0.9911\n",
            "Epoch [40], last_lr: 0.00015, train_loss: 0.9093, val_loss: 0.9118, val_acc: 0.9913\n",
            "Epoch [41], last_lr: 0.00012, train_loss: 0.9093, val_loss: 0.9120, val_acc: 0.9917\n",
            "Epoch [42], last_lr: 0.00010, train_loss: 0.9092, val_loss: 0.9121, val_acc: 0.9918\n",
            "Epoch [43], last_lr: 0.00007, train_loss: 0.9092, val_loss: 0.9120, val_acc: 0.9918\n",
            "Epoch [44], last_lr: 0.00005, train_loss: 0.9092, val_loss: 0.9120, val_acc: 0.9918\n",
            "Epoch [45], last_lr: 0.00003, train_loss: 0.9092, val_loss: 0.9121, val_acc: 0.9919\n",
            "Epoch [46], last_lr: 0.00002, train_loss: 0.9092, val_loss: 0.9122, val_acc: 0.9921\n",
            "Epoch [47], last_lr: 0.00001, train_loss: 0.9092, val_loss: 0.9122, val_acc: 0.9921\n",
            "Epoch [48], last_lr: 0.00000, train_loss: 0.9092, val_loss: 0.9121, val_acc: 0.9919\n",
            "Epoch [49], last_lr: 0.00000, train_loss: 0.9092, val_loss: 0.9121, val_acc: 0.9920\n"
          ]
        }
      ],
      "source": [
        "history = train(50, PatchModel, train_dl, val_dl, 0.001, opt_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWS5xFOwDNh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c4c4693-191f-4978-fc26-324be836ceb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved!\n"
          ]
        }
      ],
      "source": [
        "save_path = root+\"/weights/best4.pt\"\n",
        "\n",
        "torch.save(PatchModel.state_dict(), save_path)\n",
        "print(\"saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j45AzM7kS2j3"
      },
      "source": [
        "### Load weights to PatchModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdKsWs6gS7rJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49a69fa-814b-4add-ca1c-aa2a8890b109"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FinalModel(\n",
              "  (cin1): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (B11): B_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (B21): B_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (B31): B_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (R11): Residual(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (R21): Residual(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (cin2): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (B12): B_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (B22): B_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (B32): B_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (R12): Residual(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (R22): Residual(\n",
              "    (C1): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (D1): D_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 4), stride=(1, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (D2): D_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 4), stride=(1, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (D3): D_Block(\n",
              "    (C1): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C2): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (C3): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 4), stride=(1, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (tr): TransformerBlock(\n",
              "      (norm1): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "        (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (norm2): LayerNorm(\n",
              "        (body): BiasFree_LayerNorm()\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (project_in): Conv2d(32, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (dwconv): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170, bias=False)\n",
              "        (project_out): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cout): Sequential(\n",
              "    (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "PatchModel.load_state_dict(torch.load(root+'/weights/best4.pt'))\n",
        "PatchModel.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YymYzxQ5wGGO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c78763f-15b5-4d1d-de70-9c63806bf80d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_loss': 0.9102460741996765, 'val_acc': 0.9940363842928348}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "evaluate(PatchModel, val_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "H_birW9hcotn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_patch_centroid(binary_mask):\n",
        "    # print(binary_mask.shape)\n",
        "    pltimg = binary_mask.cpu().detach().numpy()\n",
        "    pltimg = np.clip(pltimg, 0, 1)\n",
        "    pltimg = np.squeeze(pltimg)\n",
        "    pltimg = np.transpose(pltimg, (2, 1, 0))\n",
        "    # print(pltimg.shape)\n",
        "    plt.imshow(pltimg, cmap='gray')\n",
        "\n",
        "    pred_patch = binary_mask.cpu().detach().numpy()\n",
        "    binary_mask = np.where(pred_patch > 0.5, 1, 0)\n",
        "\n",
        "    binary_mask_tensor = torch.tensor(binary_mask)\n",
        "    indices = torch.nonzero(binary_mask_tensor, as_tuple=False)\n",
        "    centroid = indices.float().mean(dim=0)\n",
        "    # print(centroid)\n",
        "    center_x, center_y = centroid[1].item(), centroid[2].item()\n",
        "\n",
        "    return center_x, center_y\n",
        "\n",
        "# img = [[[0, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0]],\n",
        "#        [[0, 0, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0]]]\n",
        "# img = np.array(img)\n",
        "# img = torch.tensor(img)\n",
        "# find_patch_centroid(img)"
      ],
      "metadata": {
        "id": "Bp0586Vscvwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(img1, img2):\n",
        "    img1 = cv2.imread(img1)\n",
        "    img2 = cv2.imread(img2)\n",
        "\n",
        "    if img1 is None:\n",
        "        print(\"NoneImage\")\n",
        "\n",
        "    img1 = img1.astype(np.float32)\n",
        "    img2 = img2.astype(np.float32)\n",
        "\n",
        "    cropped_img1 = torch.tensor(img1)\n",
        "    cropped_img2 = torch.tensor(img2)\n",
        "\n",
        "    cropped_img1 = torch.unsqueeze(cropped_img1,0)\n",
        "    cropped_img2 = torch.unsqueeze(cropped_img2,0)\n",
        "\n",
        "    cropped_img1 = cropped_img1.permute(0, 3, 2, 1)\n",
        "    cropped_img2 = cropped_img2.permute(0, 3, 2, 1)\n",
        "\n",
        "    cropped_img1 = to_device(device, cropped_img1)\n",
        "    cropped_img2 = to_device(device, cropped_img2)\n",
        "\n",
        "    # print(cropped_img1.cpu().detach().numpy().shape, cropped_img2.cpu().detach().numpy().shape)\n",
        "    res = PatchModel(cropped_img1, cropped_img2)\n",
        "    # print(res.shape)\n",
        "    res = res[0]\n",
        "    # print(res.shape)\n",
        "\n",
        "    sigmo = nn.Sigmoid()\n",
        "    res = sigmo(res)\n",
        "\n",
        "    x,y = find_patch_centroid(res)\n",
        "    print(\"prediction: \", x, y)"
      ],
      "metadata": {
        "id": "nmZ8xmMzcn63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF64MioYwRPv"
      },
      "source": [
        "### Testing PatchModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "VJUFf_AWwXC1",
        "outputId": "7dd08a89-f159-456e-a6cc-2405d1b94686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction:  36.92037582397461 39.323184967041016\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "img1 = root+'/testimages/patch5_0.png'\n",
        "img2 = root+'/testimages/patch1_0.png'\n",
        "\n",
        "test(img1, img2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgn_Yo1OBFTU"
      },
      "source": [
        "### Unzip the Yolo Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIszZiM8SJgP",
        "outputId": "49d56265-c63d-46b3-a9d0-78da91059e56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  drive/MyDrive/DEP/final-yolov3.zip\n",
            "replace drive/MyDrive/DEP/final-yolov3/.dockerignore? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip '{root}/final-yolov3.zip' -d '{root}/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tJaNryVC3w6"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwJlwgCGhrBQ"
      },
      "outputs": [],
      "source": [
        "#0 0.838555 0.495993\n",
        "\n",
        "# Since we have to process a video we cannot go directly, going with the previous procedure we have stored the captured\n",
        "# frames of the videos into the dataset named list\n",
        "\n",
        "# We have to use two models, so we have to go frame by frame only, we cannot use a batch\n",
        "# The frames where the accuracy of the detected ball goes beyond 80% with the Yolo model, lets say the frame be some Yth frame\n",
        "# Now we have to find the coordinates of the center of the bounding box of detected ball by the Yolo Model\n",
        "\n",
        "# Since i have the center of the ball in the Yth frame\n",
        "# Now go to the (Y+1)th frame and cut a patch of 72x72 with the same center of ball in the Yth frame\n",
        "\n",
        "# Our Architecture take two consecutive frames as the input, so feed the (Y+1)th frame as both img1 & img2\n",
        "# The architecture will process both the frames and return a binary segmented image of ball in the patch of the (y+1)th frame\n",
        "# Now the binary segmented output will be representing the ball with 1's, so we have to find the center of the 1's in the output\n",
        "# use the center to crop a patch in the (y+2)th frame and now give img1 = (y+1)th & img2 = (y+2)th as the input to the architecture\n",
        "# also use the center to fill 1's in the original (y+1)th frame\n",
        "# store all the frames in some final list\n",
        "# convert the final list of processed frames to a final video, save the video and we are done\n",
        "\n",
        "\n",
        "def cropImage(image, x1, y1):\n",
        "    ''' Feed the frame with the center (x1, y1) to get the patch '''\n",
        "\n",
        "    h, w, d = image.shape\n",
        "    if (x1<1 and y1<1):\n",
        "        x1 = x1*w\n",
        "        y1 = y1*h\n",
        "\n",
        "    x1 = int(x1)\n",
        "    y1 = int(y1)\n",
        "\n",
        "    flag = False\n",
        "    half_side = 36\n",
        "\n",
        "    # crop the image to get a patch of 70x70\n",
        "    if(y1-half_side < 0):\n",
        "        padding_rows = half_side-y1\n",
        "        cropped_image = image[0:y1+half_side, x1-half_side:x1+half_side]\n",
        "        cropped_image = np.pad(cropped_image, ((padding_rows, 0), (0, 0), (0, 0)), mode='constant', constant_values=0)\n",
        "        flag = True\n",
        "\n",
        "    if(x1-half_side < 0):\n",
        "        padding_rows = half_side-x1\n",
        "        cropped_image= image[y1-half_side:y1+half_side, 0:x1+half_side]\n",
        "        cropped_image = np.pad(cropped_image, ((0, 0),(padding_rows, 0),(0, 0)), mode='constant', constant_values=0)\n",
        "        flag = True\n",
        "\n",
        "    if(y1+half_side >= h):\n",
        "        padding_rows = h-y1\n",
        "        cropped_image = image[y1-half_side:w, x1-half_side:x1+half_side]\n",
        "        cropped_image= np.pad(cropped_image, ((0, padding_rows),(0, 0), (0, 0)), mode='constant', constant_values=0)\n",
        "        flag = True\n",
        "\n",
        "    if(x1+half_side >= w):\n",
        "        padding_rows = w-x1\n",
        "        cropped_image = image[y1-half_side:y1+half_side, x1-half_side:h]\n",
        "        cropped_image= np.pad(cropped_image, ((0, 0),(0, padding_rows),(0, 0)), mode='constant', constant_values=0)\n",
        "        flag = True\n",
        "\n",
        "    if(flag == False): cropped_image = image[y1-half_side:y1+half_side, x1-half_side:x1+half_side]\n",
        "\n",
        "    return cropped_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKgTljsOObwk"
      },
      "outputs": [],
      "source": [
        "def find_big_coordinates(x_ball, y_ball, x_patch, y_patch, image):\n",
        "    h, w, d = image.shape\n",
        "\n",
        "    if x_patch<1 and y_patch<1:\n",
        "        x_patch = x_patch*w\n",
        "        y_patch = y_patch*h\n",
        "\n",
        "    x_frame = (x_patch - 36) + x_ball\n",
        "    y_frame = (y_patch - 36) + y_ball\n",
        "\n",
        "    return x_frame, y_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAabwSNd1Doz"
      },
      "outputs": [],
      "source": [
        "def add_circle(img, x, y):\n",
        "    h, w, c = img.shape\n",
        "    x, y = int(x*w), int(y*h)\n",
        "    center = (x, y)\n",
        "    radius = 5\n",
        "    color = (255, 255, 255)\n",
        "    thickness = -1\n",
        "    print(center)\n",
        "    print(type(img))\n",
        "    print(img.shape)\n",
        "    cv2.circle(img, center, radius, color, thickness)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X7VwfbP21FL"
      },
      "outputs": [],
      "source": [
        "def save_video(final_image_list, output_folder):\n",
        "    h, w, d = final_image_list[0].shape\n",
        "\n",
        "    output_path = f'{output_folder}/out_video.mp4'\n",
        "    fourcc = cv2.VideoWriter_fourcc('m','p','4','v')\n",
        "    video = cv2.VideoWriter(output_path, fourcc, 15, (w, h))\n",
        "\n",
        "    for j in final_image_list:\n",
        "        video.write(j)\n",
        "\n",
        "    print(\"Video saved!\")\n",
        "    cv2.destroyAllWindows()\n",
        "    video.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdfNrMF_YVge"
      },
      "source": [
        "### Merge Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdV0sQSGua5b"
      },
      "outputs": [],
      "source": [
        "path = root+'/final-yolov3'\n",
        "\n",
        "def merge(dataset):\n",
        "    # Here I am initializing some variables like x,y are intially 0,0 they will be updatd once yolo_accuracy>80% and flag is to switch between yolo\n",
        "    # and architecture model ct will tell me whether its the first time or not that we'll have to use two same images in the architecture\n",
        "    # prevImage will store the 1st image that will go to the connection function and yolostart is when we have first image we dont know the accuracy we'll just apply yolo\n",
        "\n",
        "    final_img_list = []\n",
        "    flag = True         # True for YoloModel and False for PatchModel\n",
        "    patchflag = True    # True for first time and False from second time\n",
        "    x_yolo, y_yolo = 0., 0.     # center of ball given by yolo model in the complete frame\n",
        "    x_ball, y_ball, = 0., 0.    # center of ball given by patch model in the patch of a frame\n",
        "    x_patch, y_patch = 0., 0.   # center of the patch cropped\n",
        "    x_frame, y_frame = 0., 0.   # center of ball in the complete frame\n",
        "    prev_img = 0\n",
        "    accuracy_yolo=0\n",
        "\n",
        "    ball_coords = []\n",
        "\n",
        "    for i in range(len(dataset)-1):\n",
        "        img_path = dataset[i]\n",
        "\n",
        "        # YOLO model\n",
        "        if flag == True:\n",
        "            print(\"yolo\")\n",
        "            !python {path}/detect.py --weights {path}/runs/train/exp7/weights/best.pt --source {img_path} --outputs coordinates/coord\n",
        "            print(\"Yolo detection done\")\n",
        "\n",
        "            with open(\"coordinates/coord.pkl\", \"rb\") as file:\n",
        "                coords = pickle.load(file)\n",
        "\n",
        "            print(\"found coord\")\n",
        "            coords = coords[-1]\n",
        "            accuracy_yolo = coords[1]*100\n",
        "            x_yolo = (coords[2][0] + coords[2][2]) / 2\n",
        "            y_yolo = (coords[2][1] + coords[2][3]) / 2\n",
        "            print(\"coord: \", coords)\n",
        "            print(accuracy_yolo, x_yolo, y_yolo)\n",
        "\n",
        "            if accuracy_yolo >= 80:\n",
        "                print(\"accuracy greater than 80\")\n",
        "                flag = False # to switch to the different architecture\n",
        "\n",
        "            ball_coords.append( [x_yolo, y_yolo] )\n",
        "\n",
        "        # Patch Model\n",
        "        else:\n",
        "            frame = cv2.imread(img_path)\n",
        "            frame = np.array(frame)\n",
        "            frame = frame.astype(np.float32)\n",
        "\n",
        "            if patchflag == True:  # same images first time\n",
        "                print(\"Virgin PatchModel\")\n",
        "                frame2 = frame\n",
        "                cropped_img1 = cropImage(frame, x_yolo, y_yolo)\n",
        "                cropped_img2 = cropped_img1\n",
        "                print(cropped_img1.shape)\n",
        "\n",
        "                x_patch, y_patch = x_yolo, y_yolo\n",
        "                prev_img = cropped_img2\n",
        "                patchflag = False\n",
        "\n",
        "            else:   # different frames\n",
        "                print(\"Playboy PatchModel\")\n",
        "                img_path2 = dataset[i+1]\n",
        "                frame2 = cv2.imread(img_path1)\n",
        "                frame2 = np.array(frame2)\n",
        "                frame2 = frame2.astype(np.float32)\n",
        "\n",
        "                cropped_img1 = prev_img\n",
        "                cropped_img2 = cropImage(frame2, x_patch, y_patch)\n",
        "                prev_img = cropped_img2\n",
        "\n",
        "\n",
        "            print(\"Cropping Done\")\n",
        "            cropped_img1 = torch.tensor(cropped_img1)\n",
        "            cropped_img2 = torch.tensor(cropped_img2)\n",
        "            cropped_img1 = torch.unsqueeze(cropped_img1,0)\n",
        "            cropped_img2 = torch.unsqueeze(cropped_img2,0)\n",
        "\n",
        "            cropped_img1 = cropped_img1.permute(0, 3, 2, 1)\n",
        "            cropped_img2 = cropped_img2.permute(0, 3, 2, 1)\n",
        "\n",
        "            cropped_img1 = to_device(device, cropped_img1)\n",
        "            cropped_img2 = to_device(device, cropped_img2)\n",
        "\n",
        "            print(cropped_img1.shape)\n",
        "            print(cropped_img1.device)\n",
        "\n",
        "            print(\"Inserting cropped images to PatchModel\")\n",
        "            res = PatchModel(cropped_img1, cropped_img2)\n",
        "            res = torch.sigmoid(res)\n",
        "            print(res)\n",
        "            break\n",
        "\n",
        "            print(\"Got PatchModel Output\")\n",
        "            x_ball, y_ball = find_patch_centroid(res)\n",
        "            print(\"Found Patch Coordinates\")\n",
        "            x_frame, y_frame = find_big_coordinates(x_ball, y_ball, x_patch, y_patch, frame2)\n",
        "            print(\"Found Big Coordinates\")\n",
        "            print(\"centers: \", x_ball, y_ball, x_patch, y_patch, x_frame, y_frame, x_yolo, y_yolo)\n",
        "\n",
        "\n",
        "            ball_coords.append( [x_frame, y_frame] )\n",
        "            x_patch, y_patch = x_frame, y_frame\n",
        "\n",
        "\n",
        "    # Appending final ball coordinates\n",
        "    with open(\"final_coords.pkl\", \"wb\") as file:\n",
        "        pickle.load(ball_coords, file)\n",
        "\n",
        "    # returning the final_img_list\n",
        "    return ball_coords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e47i3OBVT9vW"
      },
      "source": [
        "### Dataset Preparation for Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhwG4F2eJpim"
      },
      "outputs": [],
      "source": [
        "# Program To Read video and Extract Frames\n",
        "def FrameCapture(path):\n",
        "    img_folder = \"images\"\n",
        "    if not os.path.exists(img_folder):\n",
        "        os.makedirs(img_folder)\n",
        "\n",
        "    vidObj = cv2.VideoCapture(path)\n",
        "    success = 1\n",
        "    count = 0\n",
        "    cnt = True\n",
        "\n",
        "    while success:\n",
        "        success, image = vidObj.read()\n",
        "        if cnt == True:\n",
        "            h, w, c = image.shape\n",
        "            cnt = False\n",
        "\n",
        "        if image is None: continue\n",
        "        if(image.shape == (h, w, c)):\n",
        "            cv2.imwrite(\"images/frame%d.jpg\" % count, image)\n",
        "            count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo1eqGMXtqVN",
        "outputId": "261f9912-d42c-4775-aec4-7075e5440183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['unseen_output_video.mp4']\n",
            "15\n",
            "images/frame1.jpg\n"
          ]
        }
      ],
      "source": [
        "data_folder = root+\"/test_videos\"\n",
        "videos_list = [v for v in os.listdir(data_folder) if v.lower().endswith(('.mp4'))]\n",
        "print(videos_list)\n",
        "\n",
        "# Read the videos and save the frame to images folder\n",
        "for video_name in videos_list:\n",
        "    video_path = os.path.join(data_folder, video_name)\n",
        "    FrameCapture(video_path)\n",
        "\n",
        "dataset = ['images/'+p for p in os.listdir('images')]\n",
        "\n",
        "print(len(dataset))\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzRutD-eUWqu"
      },
      "source": [
        "### Run Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WNtEeajZfdbc",
        "outputId": "94e9c47b-402d-47bd-ce52-80daf9b99f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yolo\n",
            "\u001b[34m\u001b[1mdetect: \u001b[0moutputs=coordinates/coord, weights=['drive/MyDrive/DEP/final-yolov3/runs/train/exp7/weights/best.pt'], source=images/frame1.jpg, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=True, save_crop=True, nosave=False, classes=None, agnostic_nms=False, augment=True, visualize=True, update=True, project=drive/.shortcut-targets-by-id/15pTYB9TpP62KZRTjPT5VylHGPbYvlLJk/DEP/final-yolov3/runs/detect, name=new, exist_ok=False, line_thickness=1, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "Expected end or semicolon (after name and no valid version specifier)\n",
            "    alabaster=0.7.12=py37_0\n",
            "             ^\n",
            "YOLOR 🚀 v9.6.0-22-g0bbd055 torch 2.2.1+cu121 CUDA:0 (Tesla T4, 15102.0625MB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 333 layers, 61949149 parameters, 0 gradients\n",
            "image 1/1 /content/images/frame1.jpg: 640x640 1 sports ball, Done. (0.191s)\n",
            "dataset mode:  image\n",
            "save_path runs/detect/newdetections/frame1.jpg\n",
            "Saving the outputs!!!\n",
            "out_dir:  coordinates\n",
            "Saving Done!!!\n",
            "Speed: 8.7ms pre-process, 190.9ms inference, 655.4ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mdrive/.shortcut-targets-by-id/15pTYB9TpP62KZRTjPT5VylHGPbYvlLJk/DEP/final-yolov3/runs/detect/new45\u001b[0m\n",
            "1 labels saved to drive/.shortcut-targets-by-id/15pTYB9TpP62KZRTjPT5VylHGPbYvlLJk/DEP/final-yolov3/runs/detect/new45/labels\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 541, in _check_seekable\n",
            "    f.seek(f.tell())\n",
            "AttributeError: 'list' object has no attribute 'seek'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/DEP/final-yolov3/detect.py\", line 280, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/drive/MyDrive/DEP/final-yolov3/detect.py\", line 274, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/DEP/final-yolov3/detect.py\", line 234, in run\n",
            "    strip_optimizer(weights)  # update model (to fix SourceChangeWarning)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/15pTYB9TpP62KZRTjPT5VylHGPbYvlLJk/DEP/final-yolov3/utils/general.py\", line 736, in strip_optimizer\n",
            "    x = torch.load(f, map_location=torch.device('cpu'))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 998, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 450, in _open_file_like\n",
            "    return _open_buffer_reader(name_or_buffer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 435, in __init__\n",
            "    _check_seekable(buffer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 544, in _check_seekable\n",
            "    raise_err_msg([\"seek\", \"tell\"], e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 537, in raise_err_msg\n",
            "    raise type(e)(msg)\n",
            "AttributeError: 'list' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.\n",
            "Yolo detection done\n",
            "found coord\n",
            "coord:  [32, 0.8665991425514221, [959, 502, 969, 512]]\n",
            "86.65991425514221 964.0 507.0\n",
            "accuracy greater than 80\n",
            "Virgin PatchModel\n",
            "(72, 72, 3)\n",
            "Cropping Done\n",
            "torch.Size([1, 3, 72, 72])\n",
            "cuda:0\n",
            "Inserting cropped images to PatchModel\n",
            "tensor([[[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
            "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
            "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
            "          ...,\n",
            "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
            "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
            "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],\n",
            "\n",
            "         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
            "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
            "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
            "          ...,\n",
            "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
            "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
            "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],\n",
            "\n",
            "         [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
            "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
            "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
            "          ...,\n",
            "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
            "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
            "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],\n",
            "       device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "load() takes exactly 1 positional argument (2 given)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-07c86a71a70c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mball_coords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-105-d87dfcb1f08c>\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m# Appending final ball coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final_coords.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mball_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# returning the final_img_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: load() takes exactly 1 positional argument (2 given)"
          ]
        }
      ],
      "source": [
        "ball_coords = merge(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMdnA-ay9Rcu"
      },
      "outputs": [],
      "source": [
        "# Add white circle to frames in place of ball\n",
        "final_img_list = [add_circle(dataset[i], ball_coords[i][0], ball_coords[i][1]) for i in range(len(ball_coords))]\n",
        "%rm -r \"images\"\n",
        "\n",
        "# Convert frames to Video and save to output_folder\n",
        "output_folder = \"output\"\n",
        "save_video(final_img_list, output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Patch"
      ],
      "metadata": {
        "id": "UpjR6bTeufQB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGau7amUn0tv"
      },
      "outputs": [],
      "source": [
        "def test_patch(dataset):\n",
        "    patchflag = True\n",
        "    x_yolo, y_yolo = 0., 0.     # center of ball given by yolo model in the complete frame\n",
        "    x_ball, y_ball, = 0., 0.    # center of ball given by patch model in the patch of a frame\n",
        "    x_patch, y_patch = 0., 0.   # center of the patch cropped\n",
        "    x_frame, y_frame = 0., 0.   # center of ball in the complete frame\n",
        "\n",
        "    for i in range(len(dataset)-1):\n",
        "        img_path = dataset[i]\n",
        "        frame = cv2.imread(img_path)\n",
        "        frame = np.array(frame)\n",
        "        frame = frame.astype(np.float32)\n",
        "\n",
        "        if patchflag == True:  # same images first time\n",
        "            print(\"Virgin PatchModel\")\n",
        "            frame2 = frame\n",
        "            cropped_img1 = cropImage(frame, x_yolo, y_yolo)\n",
        "            cropped_img2 = cropped_img1\n",
        "            print(cropped_img1.shape)\n",
        "\n",
        "            x_patch, y_patch = x_yolo, y_yolo\n",
        "            prev_img = cropped_img2\n",
        "            patchflag = False\n",
        "\n",
        "        else:   # different frames\n",
        "            print(\"Playboy PatchModel\")\n",
        "            img_path2 = dataset[i+1]\n",
        "            frame2 = cv2.imread(img_path2)\n",
        "            frame2 = np.array(frame2)\n",
        "            frame2 = frame2.astype(np.float32)\n",
        "\n",
        "            cropped_img1 = prev_img\n",
        "            cropped_img2 = cropImage(frame2, x_patch, y_patch)\n",
        "            prev_img = cropped_img2\n",
        "\n",
        "\n",
        "        print(\"Cropping Done\")\n",
        "        cropped_img1 = torch.tensor(cropped_img1)\n",
        "        cropped_img2 = torch.tensor(cropped_img2)\n",
        "        cropped_img1 = torch.unsqueeze(cropped_img1,0)\n",
        "        cropped_img2 = torch.unsqueeze(cropped_img2,0)\n",
        "\n",
        "        cropped_img1 = cropped_img1.permute(0, 3, 2, 1)\n",
        "        cropped_img2 = cropped_img2.permute(0, 3, 2, 1)\n",
        "\n",
        "        cropped_img1 = to_device(device, cropped_img1)\n",
        "        cropped_img2 = to_device(device, cropped_img2)\n",
        "\n",
        "        print(cropped_img1.shape)\n",
        "        print(cropped_img1.device)\n",
        "\n",
        "        print(\"Inserting cropped images to PatchModel\")\n",
        "        res = PatchModel(cropped_img1, cropped_img2)\n",
        "        res = torch.sigmoid(res)\n",
        "\n",
        "        print(\"Got PatchModel Output\")\n",
        "        x_ball, y_ball = find_patch_centroid(res)\n",
        "        print(\"Found Patch Coordinates\")\n",
        "        x_frame, y_frame = find_big_coordinates(x_ball, y_ball, x_patch, y_patch, frame2)\n",
        "        print(\"Found Big Coordinates\")\n",
        "        print(\"centers: \", x_ball, y_ball, x_patch, y_patch, x_frame, y_frame, x_yolo, y_yolo)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "T-6z8jYiW0RO",
        "jgRVHXh_CjIq",
        "63YAWfCRJWkA",
        "XQSHu_1RJb6T",
        "XeouJyNSJhvv",
        "VwA51A0k2qv4",
        "miZT59ro1pPh",
        "zrrg75Ir8DA7",
        "G6JDJ6cw8F8D",
        "uGXnE_ENVjao",
        "YhQJ3a4czoQX",
        "wgn_Yo1OBFTU"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}